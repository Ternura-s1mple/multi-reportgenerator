# backend/llm_services.py

import os
from openai import AsyncOpenAI
import asyncio
from dotenv import load_dotenv
import google.generativeai as genai

load_dotenv()

# OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–
openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# é…ç½® Gemini API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))


async def generate_with_openai(model_name: str, topic: str):
    # æ–°å¢è°ƒè¯•ä¿¡æ¯: æ‰“å°å¼€å§‹è°ƒç”¨çš„ä¿¡æ¯
    print(f"ğŸš€ [OpenAI] å¼€å§‹è°ƒç”¨æ¨¡å‹: {model_name}...")
    try:
        prompt = f"ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è¡Œä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸»é¢˜ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„æ¸…æ™°ã€å†…å®¹è¯¦å°½çš„Markdownæ ¼å¼åˆ†ææŠ¥å‘Šã€‚ä¸»é¢˜ï¼š'{topic}'"
        response = await openai_client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        content = response.choices[0].message.content
        # æ–°å¢è°ƒè¯•ä¿¡æ¯: æ‰“å°æˆåŠŸä¿¡æ¯
        print(f"âœ… [OpenAI] æ¨¡å‹ {model_name} è°ƒç”¨æˆåŠŸã€‚")
        return {"model_name": f"OpenAI-{model_name}", "content": content}
    except Exception as e:
        # æ–°å¢è°ƒè¯•ä¿¡æ¯: æ‰“å°å¤±è´¥ä¿¡æ¯
        print(f"âŒ [OpenAI] æ¨¡å‹ {model_name} è°ƒç”¨å¤±è´¥: {e}")
        return {"model_name": f"OpenAI-{model_name}", "content": f"è°ƒç”¨å¤±è´¥: {e}"}


async def generate_with_gemini(model_name: str, topic: str):
    # æ–°å¢è°ƒè¯•ä¿¡æ¯: æ‰“å°å¼€å§‹è°ƒç”¨çš„ä¿¡æ¯
    print(f"ğŸš€ [Gemini] å¼€å§‹è°ƒç”¨æ¨¡å‹: {model_name}...")
    try:
        model = genai.GenerativeModel(model_name)
        prompt = f"ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è¡Œä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸»é¢˜ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„æ¸…æ™°ã€å†…å®¹è¯¦å°½çš„Markdownæ ¼å¼åˆ†ææŠ¥å‘Šã€‚ä¸»é¢˜ï¼š'{topic}'"
        response = await model.generate_content_async(prompt)
        content = response.text
        # æ–°å¢è°ƒè¯•ä¿¡æ¯: æ‰“å°æˆåŠŸä¿¡æ¯
        print(f"âœ… [Gemini] æ¨¡å‹ {model_name} è°ƒç”¨æˆåŠŸã€‚")
        return {"model_name": f"Gemini-{model_name}", "content": content}
    except Exception as e:
        # æ–°å¢è°ƒè¯•ä¿¡æ¯: æ‰“å°å¤±è´¥ä¿¡æ¯
        print(f"âŒ [Gemini] æ¨¡å‹ {model_name} è°ƒç”¨å¤±è´¥: {e}")
        return {"model_name": f"Gemini-{model_name}", "content": f"è°ƒç”¨å¤±è´¥: {e}"}


async def mock_llm_call(model_name: str, topic: str, delay: float = 1):
    print(f"ğŸš€ [Mock] å¼€å§‹è°ƒç”¨æ¨¡å‹: {model_name}...")
    await asyncio.sleep(delay)
    print(f"âœ… [Mock] æ¨¡å‹ {model_name} è°ƒç”¨å®Œæˆã€‚")
    return {
        "model_name": model_name,
        "content": f"# {topic} - ç”± {model_name} (æ¨¡æ‹Ÿ) ç”Ÿæˆ\n\nè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹ŸæŠ¥å‘Šï¼Œç”¨äºæµ‹è¯•å’Œå±•ç¤ºã€‚"
    }